{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating Custom Cost Functions</h1>\n",
    "\n",
    "In this tutorial, we show how to create a custom cost function that might be needed for an application. While we can always use the `AutoDiffCostFunction` by simply writing an error function, it is often more efficient for compute-intensive applications to derive a new `CostFunction` subclass and use closed-form Jacobians. \n",
    "\n",
    "We will show how to write a custom `VectorDifference` cost function in this tutorial. This cost function provides the difference between two `Vector`s as the error. \n",
    "\n",
    "Note: `VectorDifference` is a simplified version of the `Difference` cost function already provided in the Theseus library, and shown in Tutorial 0. `Difference` can be used on any LieGroup, while `VectorDifference` can only be used on Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialization</h2> \n",
    "\n",
    "Any `CostFunction` subclass should be initialized with a `CostWeight` and all arguments needed to compute the cost function. In this example, we set up `__init__` function for `VectorDifference` to require as input the two `Vector`s whose difference we wish to compute: the `Vector` to be optimized, `var`, and the `Vector` that is the reference for comparison, `target`. \n",
    "\n",
    "In addition, the `__init__` function also needs to register the optimization variables and all the auxiliary variables. In this example, optimization variable `var` is registered with `register_optim_vars`. The other input necessary to evaluate the cost, `target` is registered with `register_aux_vars`. This is required for the nonlinear optimizers to work correctly: these functions register the optimization and auxiliary variables into internal lists, and then are easily used by the relevant `Objective` to add them, ensure no name collisions, and to update them with new values.\n",
    "\n",
    "The `CostWeight` is used to weight the errors and jacobians, and is required by every `CostFunction` sub-class (the error and jacobian weighting functions are inherited from the parent `CostFunction` class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.474667Z",
     "start_time": "2023-02-09T04:54:04.903036Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import theseus as th\n",
    "\n",
    "class VectorDifference(th.CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_weight: th.CostWeight,\n",
    "        var: th.Vector,\n",
    "        target: th.Vector,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name) \n",
    "\n",
    "        # add checks to ensure the input arguments are of the same class and dof:\n",
    "        if not isinstance(var, target.__class__):\n",
    "            raise ValueError(\n",
    "                \"Variable for the VectorDifference inconsistent with the given target.\"\n",
    "            )\n",
    "        if not var.dof() == target.dof():\n",
    "            raise ValueError(\n",
    "                \"Variable and target in the VectorDifference must have identical dof.\"\n",
    "            )\n",
    "\n",
    "        self.var = var\n",
    "        self.target = target\n",
    "\n",
    "        # register variable and target\n",
    "        self.register_optim_vars([\"var\"])\n",
    "        self.register_aux_vars([\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implement abstract functions</h2> \n",
    "\n",
    "Next, we need to implement the abstract functions of `CostFunction`: `dim`, `error`, `jacobians`, and `_copy_impl`:\n",
    "- `dim`: returns the degrees of freedom (`dof`) of the error; in this case, this is the `dof` of the optimization variable `var`\n",
    "- `error`: returns the difference of Vectors i.e. `var` - `target`\n",
    "- `jacobian`: returns the Jacobian of the error with respect to the `var`\n",
    "- `_copy_impl`: creates a deep copy of the internal class members\n",
    "\n",
    "We illustrate these below (including once again the `__init__` function from above, so the class is fully defined.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.482105Z",
     "start_time": "2023-02-09T04:54:05.476902Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class VectorDifference(th.CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_weight: th.CostWeight,\n",
    "        var: th.Vector,\n",
    "        target: th.Vector,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name) \n",
    "        self.var = var\n",
    "        self.target = target\n",
    "        # to improve readability, we have skipped the data checks from code block above\n",
    "        self.register_optim_vars([\"var\"])\n",
    "        self.register_aux_vars([\"target\"])\n",
    "\n",
    "    def error(self) -> torch.Tensor:\n",
    "        return (self.var - self.target).tensor\n",
    "\n",
    "    def jacobians(self) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
    "        return [\n",
    "            # jacobian of error function wrt var is identity matrix I\n",
    "            torch.eye(self.dim(), dtype=self.var.dtype)  \n",
    "            # repeat jacobian across each element in the batch\n",
    "            .repeat(self.var.shape[0], 1, 1)  \n",
    "            # send to variable device\n",
    "            .to(self.var.device)  \n",
    "        ], self.error()\n",
    "\n",
    "    def dim(self) -> int:\n",
    "        return self.var.dof()\n",
    "\n",
    "    def _copy_impl(self, new_name: Optional[str] = None) -> \"VectorDifference\":\n",
    "        return VectorDifference(  # type: ignore\n",
    "            self.var.copy(), self.weight.copy(), self.target.copy(), name=new_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Usage</h2>\n",
    "\n",
    "We show now that the `VectorDifference` cost function works as expected. \n",
    "\n",
    "For this, we create a set of `VectorDifference` cost functions each over a pair of `Vector`s <i>a_i</i> and <i>b_i</i>, and add them to an `Objective`. We then create the data for each `Vector` <i>a_i</i> and <i>b_i</i> of the `VectorDifference` cost functions, and `update` the `Objective` with it. The code snippet below shows that the `Objective` error is correctly computed.\n",
    "\n",
    "We use a `ScaleCostWeight` as the input `CostWeight` here: this is a scalar real-valued `CostWeight` used to weight the `CostFunction`; for simplicity we use a fixed value of 1. in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.506687Z",
     "start_time": "2023-02-09T04:54:05.483181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample error squared norm: 20.0\n"
     ]
    }
   ],
   "source": [
    "cost_weight = th.ScaleCostWeight(1.0)\n",
    "\n",
    "# construct cost functions and add to objective\n",
    "objective = th.Objective()\n",
    "num_test_fns = 10\n",
    "for i in range(num_test_fns):\n",
    "    a = th.Vector(2, name=f\"a_{i}\")\n",
    "    b = th.Vector(2, name=f\"b_{i}\")\n",
    "    cost_fn = VectorDifference(cost_weight, a, b)\n",
    "    objective.add(cost_fn)\n",
    "    \n",
    "# create data for adding to the objective\n",
    "theseus_inputs = {}\n",
    "for i in range(num_test_fns):\n",
    "    # each pair of var/target has a difference of [1, 1]\n",
    "    theseus_inputs.update({f\"a_{i}\": torch.ones((1,2)), f\"b_{i}\": 2 * torch.ones((1,2))})\n",
    "\n",
    "objective.update(theseus_inputs)\n",
    "# sum of squares of errors [1, 1] for 10 cost fns: the result should be 20\n",
    "error_sq = objective.error_squared_norm()\n",
    "print(f\"Sample error squared norm: {error_sq.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto check Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your hand-written Jacobian by comparing the Jacobian results from AutoDiffCostFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.518942Z",
     "start_time": "2023-02-09T04:54:05.507768Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import theseus as th\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, Union, cast\n",
    "from theseus.core import CostFunction, CostWeight, Variable, as_variable\n",
    "from theseus.geometry import LieGroup, Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cost function define a simple problem as below: Assuming you know a certain number of the base stations and their position, and you also got the distance between the user and the base station. Then you can calculate the user location.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.529471Z",
     "start_time": "2023-02-09T04:54:05.519695Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateInput(SampleNum = 100):\n",
    "\n",
    "    BasePos_x = th.Variable(torch.randint(100,(SampleNum,3),dtype = torch.float64), name=\"BasePos_x\")\n",
    "    BasePos_y = th.Variable(torch.randint(100,(SampleNum,3),dtype = torch.float64), name=\"BasePos_y\")\n",
    "    Pos_x = th.Variable(torch.randint(100,(SampleNum,1),dtype = torch.float64), name=\"Pos_x\")\n",
    "    Pos_y = th.Variable(torch.randint(100,(SampleNum,1),dtype = torch.float64), name=\"Pos_y\")\n",
    "    Distance_x = BasePos_x.tensor - Pos_x.tensor\n",
    "    Distance_y = BasePos_y.tensor - Pos_y.tensor\n",
    "    #Distance = torch.sum((BasePos_x.tensor - Pos_x.tensor).square() + (BasePos_y.tensor - Pos_y.tensor).square(), dim = 1, keepdim = True).sqrt()\n",
    "    Distance = (Distance_x.square() + Distance_y.square()).sqrt()\n",
    "    Range = th.Variable(Distance, name = \"Range\")\n",
    "    \n",
    "    \n",
    "    return Range, BasePos_x, BasePos_y\n",
    "\n",
    "\n",
    "def test_jacobian_Cost(Pos_x, Pos_y, Range, BasePos_x, BasePos_y, cost_weight, name):\n",
    "\n",
    "    cost_function = toyCost(Pos_x, Pos_y, Range, BasePos_x, BasePos_y, cost_weight)\n",
    "\n",
    "    aux_vars = Range,BasePos_x,BasePos_y\n",
    "    optim_vars = Pos_x, Pos_y\n",
    "    iteration = Range.tensor.shape[1]\n",
    "\n",
    "    cost_function_numeric = th.AutoDiffCostFunction(optim_vars, cost_function.err_func_numeric, iteration, aux_vars=aux_vars)\n",
    "    \n",
    "    expected_jacs, error_jac = cost_function_numeric.jacobians()\n",
    "    #print(expected_jacs)\n",
    "    jacobians, error_jac = cost_function.jacobians()\n",
    "    #print(jacobians)\n",
    "    error = cost_function.error()\n",
    "    print(\"checking Jacobian for Cost...\")\n",
    "    \n",
    "    for i in range(len(jacobians)):\n",
    "        assert torch.allclose(jacobians[i], expected_jacs[i], atol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.544700Z",
     "start_time": "2023-02-09T04:54:05.530174Z"
    }
   },
   "outputs": [],
   "source": [
    "class toyCost(CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        Pos_x: Vector,\n",
    "        Pos_y: Vector,\n",
    "\n",
    "        Range: Vector, \n",
    "        BasePos_x: Vector,\n",
    "        BasePos_y: Vector,\n",
    "\n",
    "\n",
    "        cost_weight: CostWeight,\n",
    "        \n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name)\n",
    "     \n",
    "\n",
    "        self.Pos_x = Pos_x\n",
    "        self.Pos_y = Pos_y\n",
    "        \n",
    "        self.Range = as_variable(Range)\n",
    "        self.BasePos_x = as_variable(BasePos_x)\n",
    "        self.BasePos_y = as_variable(BasePos_y)\n",
    "\n",
    "        self.register_optim_vars([\"Pos_x\", \"Pos_y\"])\n",
    "       \n",
    "        self.register_aux_vars([\"Range\", \"BasePos_x\", \"BasePos_y\"])\n",
    "        self.weight = cost_weight\n",
    "\n",
    "\n",
    "    # Add this function for autochecking Jocobian. What you need to do is to warp the inputs to two types: optim_vars and aux_vars    \n",
    "    def err_func_numeric(self, optim_vars, aux_vars):\n",
    "        Pos_x,Pos_y = optim_vars\n",
    "        Pos_x = Pos_x.tensor\n",
    "        Pos_y = Pos_y.tensor\n",
    "        \n",
    "        Range, BasePos_x, BasePos_y = aux_vars\n",
    "        Range = Range.tensor\n",
    "        BasePos_x = BasePos_x.tensor\n",
    "        BasePos_y = BasePos_y.tensor\n",
    "        \n",
    "        # Copy it from the self-defined cost function _err_func\n",
    "        distance_0 = ((Pos_x - BasePos_x).square()+ (Pos_y - BasePos_y).square()).sqrt()\n",
    "\n",
    "\n",
    "        err = distance_0 - Range\n",
    "\n",
    "        return err\n",
    "    \n",
    "    \n",
    "    def _err_func(self):\n",
    "        \n",
    "        Pos_x = self.Pos_x.tensor\n",
    "        Pos_y = self.Pos_y.tensor\n",
    "        Range = self.Range.tensor\n",
    "        BasePos_x = self.BasePos_x.tensor\n",
    "        BasePos_y = self.BasePos_y.tensor\n",
    "      \n",
    "        distance_0 = ((Pos_x - BasePos_x).square()+ (Pos_y - BasePos_y).square()).sqrt()\n",
    "\n",
    "\n",
    "        err = distance_0 - Range\n",
    "        print(err.shape)\n",
    "       \n",
    "        #print(err_0.shape)\n",
    "        return err\n",
    "    \n",
    "    \n",
    "    def dim(self):\n",
    "        return BasePos_x.shape[1]\n",
    "\n",
    "    def error(self) -> torch.Tensor:\n",
    "        return self._err_func()\n",
    "\n",
    "    def jacobians(self) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
    "        \n",
    "        batch_size = self.Range.shape[0]\n",
    "        \n",
    "        \n",
    "        dtype = self.Range.dtype\n",
    "        device = self.Range.device\n",
    "        \n",
    "\n",
    "        \n",
    "        Pos_x = self.Pos_x.tensor\n",
    "        Pos_y = self.Pos_y.tensor\n",
    "        \n",
    "\n",
    "        Range = self.Range.tensor\n",
    "        BasePos_x = self.BasePos_x.tensor\n",
    "        BasePos_y = self.BasePos_y.tensor\n",
    "        dof = BasePos_x.shape[1]\n",
    "        \n",
    "        interTerm = (Pos_x - BasePos_x).square()+ (Pos_y - BasePos_y).square()\n",
    "        \n",
    "        \n",
    "        Jacob_Pos_x = torch.zeros(batch_size, dof, 1, dtype=dtype, device=device)\n",
    "        Jacob_Pos_y = torch.zeros(batch_size, dof, 1, dtype=dtype, device=device)\n",
    "        \n",
    "        dErr_x = torch.pow(interTerm, -0.5) * (Pos_x - BasePos_x)\n",
    "\n",
    "        dErr_y = torch.pow(interTerm, -0.5) * (Pos_y - BasePos_y)\n",
    "       \n",
    "        Jacob_Pos_x[:,:,0] = dErr_x\n",
    "        Jacob_Pos_y[:,:,0] = dErr_y\n",
    "        \n",
    "        error = self.error()\n",
    "        \n",
    "        return [Jacob_Pos_x, Jacob_Pos_y], error\n",
    "\n",
    "    def _copy_impl(self, new_name: Optional[str] = None) -> \"toyCost\":\n",
    "        return toyCost(\n",
    "            self.Pos_x.copy(),\n",
    "            self.Pos_y.copy(),\n",
    "            self.Range.copy(),\n",
    "            self.BasePos_x.copy(),\n",
    "            self.BasePos_y.copy(),\n",
    "            self.weight.copy(),\n",
    "            name=new_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.558914Z",
     "start_time": "2023-02-09T04:54:05.545449Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.568749Z",
     "start_time": "2023-02-09T04:54:05.560334Z"
    }
   },
   "outputs": [],
   "source": [
    "numSample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.579162Z",
     "start_time": "2023-02-09T04:54:05.569872Z"
    }
   },
   "outputs": [],
   "source": [
    "Range, BasePos_x, BasePos_y = generateInput(numSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.594343Z",
     "start_time": "2023-02-09T04:54:05.581451Z"
    }
   },
   "outputs": [],
   "source": [
    "Pos_x = th.Vector(tensor=torch.zeros(numSample, 1), name=\"Pos_x\")\n",
    "Pos_y = th.Vector(tensor=torch.zeros(numSample, 1), name=\"Pos_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.604498Z",
     "start_time": "2023-02-09T04:54:05.595577Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_weight = th.ScaleCostWeight(1.0, name = \"CostWeight\")\n",
    "objective = toyCost(Pos_x, Pos_y, Range, BasePos_x, BasePos_y, cost_weight, name = \"cost0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T04:54:05.616545Z",
     "start_time": "2023-02-09T04:54:05.605504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3])\n",
      "torch.Size([100, 3])\n",
      "checking Jacobian for Cost...\n"
     ]
    }
   ],
   "source": [
    "test_jacobian_Cost(Pos_x, Pos_y, Range, BasePos_x, BasePos_y, cost_weight, name = \"cost0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d06c554396a5eba28728833f301e794fa84669cf40517768fd940f3df56e77b3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
