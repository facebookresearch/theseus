{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating Custom Cost Functions</h1>\n",
    "\n",
    "In this tutorial, we show how to create a custom cost function that might be needed for an application. While we can always use the `AutoDiffCostFunction` by simply writing an error function, it is often more efficient for compute-intensive applications to derive a new `CostFunction` subclass and use closed-form Jacobians. \n",
    "\n",
    "We will show how to write a custom `VectorDifference` cost function in this tutorial. This cost function provides the difference between two `Vector`s as the error. \n",
    "\n",
    "Note: `VectorDifference` is a simplified version of the `Difference` cost function already provided in the Theseus library, and shown in Tutorial 0. `Difference` can be used on any LieGroup, while `VectorDifference` can only be used on Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialization</h2> \n",
    "\n",
    "Any `CostFunction` subclass should be initialized with a `CostWeight` and all arguments needed to compute the cost function. In this example, we set up `__init__` function for `VectorDifference` to require as input the two `Vector`s whose difference we wish to compute: the `Vector` to be optimized, `var`, and the `Vector` that is the reference for comparison, `target`. \n",
    "\n",
    "In addition, the `__init__` function also needs to register the optimization variables and all the auxiliary variables. In this example, optimization variable `var` is registered with `register_optim_vars`. The other input necessary to evaluate the cost, `target` is registered with `register_aux_vars`. This is required for the nonlinear optimizers to work correctly: these functions register the optimization and auxiliary variables into internal lists, and then are easily used by the relevant `Objective` to add them, ensure no name collisions, and to update them with new values.\n",
    "\n",
    "The `CostWeight` is used to weight the errors and jacobians, and is required by every `CostFunction` sub-class (the error and jacobian weighting functions are inherited from the parent `CostFunction` class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import theseus as th\n",
    "\n",
    "class VectorDifference(th.CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_weight: th.CostWeight,\n",
    "        var: th.Vector,\n",
    "        target: th.Vector,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name) \n",
    "\n",
    "        # add checks to ensure the input arguments are of the same class and dof:\n",
    "        if not isinstance(var, target.__class__):\n",
    "            raise ValueError(\n",
    "                \"Variable for the VectorDifference inconsistent with the given target.\"\n",
    "            )\n",
    "        if not var.dof() == target.dof():\n",
    "            raise ValueError(\n",
    "                \"Variable and target in the VectorDifference must have identical dof.\"\n",
    "            )\n",
    "\n",
    "        self.var = var\n",
    "        self.target = target\n",
    "\n",
    "        # register variable and target\n",
    "        self.register_optim_vars([\"var\"])\n",
    "        self.register_aux_vars([\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implement abstract functions</h2> \n",
    "\n",
    "Next, we need to implement the abstract functions of `CostFunction`: `dim`, `error`, `jacobians`, and `_copy_impl`:\n",
    "- `dim`: returns the degrees of freedom (`dof`) of the error; in this case, this is the `dof` of the optimization variable `var`\n",
    "- `error`: returns the difference of Vectors i.e. `var` - `target`\n",
    "- `jacobian`: returns the Jacobian of the error with respect to the `var`\n",
    "- `_copy_impl`: creates a deep copy of the internal class members\n",
    "\n",
    "We illustrate these below (including once again the `__init__` function from above, so the class is fully defined.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class VectorDifference(th.CostFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_weight: th.CostWeight,\n",
    "        var: th.Vector,\n",
    "        target: th.Vector,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(cost_weight, name=name) \n",
    "        self.var = var\n",
    "        self.target = target\n",
    "        # to improve readability, we have skipped the data checks from code block above\n",
    "        self.register_optim_vars([\"var\"])\n",
    "        self.register_aux_vars([\"target\"])\n",
    "\n",
    "    def error(self) -> torch.Tensor:\n",
    "        return (self.var - self.target).tensor\n",
    "\n",
    "    def jacobians(self) -> Tuple[List[torch.Tensor], torch.Tensor]:\n",
    "        return [\n",
    "            # jacobian of error function wrt var is identity matrix I\n",
    "            torch.eye(self.dim(), dtype=self.var.dtype)  \n",
    "            # repeat jacobian across each element in the batch\n",
    "            .repeat(self.var.shape[0], 1, 1)  \n",
    "            # send to variable device\n",
    "            .to(self.var.device)  \n",
    "        ], self.error()\n",
    "\n",
    "    def dim(self) -> int:\n",
    "        return self.var.dof()\n",
    "\n",
    "    def _copy_impl(self, new_name: Optional[str] = None) -> \"VectorDifference\":\n",
    "        return VectorDifference(  # type: ignore\n",
    "            self.var.copy(), self.weight.copy(), self.target.copy(), name=new_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Usage</h2>\n",
    "\n",
    "We show now that the `VectorDifference` cost function works as expected. \n",
    "\n",
    "For this, we create a set of `VectorDifference` cost functions each over a pair of `Vector`s <i>a_i</i> and <i>b_i</i>, and add them to an `Objective`. We then create the data for each `Vector` <i>a_i</i> and <i>b_i</i> of the `VectorDifference` cost functions, and `update` the `Objective` with it. The code snippet below shows that the `Objective` error is correctly computed.\n",
    "\n",
    "We use a `ScaleCostWeight` as the input `CostWeight` here: this is a scalar real-valued `CostWeight` used to weight the `CostFunction`; for simplicity we use a fixed value of 1. in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample error squared norm: 20.0\n"
     ]
    }
   ],
   "source": [
    "cost_weight = th.ScaleCostWeight(1.0)\n",
    "\n",
    "# construct cost functions and add to objective\n",
    "objective = th.Objective()\n",
    "num_test_fns = 10\n",
    "for i in range(num_test_fns):\n",
    "    a = th.Vector(2, name=f\"a_{i}\")\n",
    "    b = th.Vector(2, name=f\"b_{i}\")\n",
    "    cost_fn = VectorDifference(cost_weight, a, b)\n",
    "    objective.add(cost_fn)\n",
    "    \n",
    "# create data for adding to the objective\n",
    "theseus_inputs = {}\n",
    "for i in range(num_test_fns):\n",
    "    # each pair of var/target has a difference of [1, 1]\n",
    "    theseus_inputs.update({f\"a_{i}\": torch.ones((1,2)), f\"b_{i}\": 2 * torch.ones((1,2))})\n",
    "\n",
    "objective.update(theseus_inputs)\n",
    "# sum of squares of errors [1, 1] for 10 cost fns: the result should be 20\n",
    "error_sq = objective.error_metric()\n",
    "print(f\"Sample error squared norm: {error_sq.item()}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d06c554396a5eba28728833f301e794fa84669cf40517768fd940f3df56e77b3"
  },
  "kernelspec": {
   "display_name": "Theseus (backward)",
   "language": "python",
   "name": "theseus_backward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
